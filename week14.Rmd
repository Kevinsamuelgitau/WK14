---
title: "Samuel Gitau"
output: Week 14 IP
---


## Loading the dataset ----
```{r}
library(readr)
df <- read_csv("C:/Users/user/Downloads/Supermarket_Dataset_1 - Sales Data.csv")

```
###Previewing the first 6 rows 
```{r}
head(df)
```
###Previewing the last 6 rows
```{r}
tail(df)
```
#checking shape of the dataset
```{r}
dim(df)
```
## Checking Information  our dataset----
###Checking number of columns 
```{r}
colnames(df)
```
```{r}
install.packages("tibble")
library(tibble)
df <- as.tibble(df)
df
```
###Checking summary statistics of our dataset
```{r}
summary(df)
```
## Cleaning the Dataset----
###Check for missing values
```{r}
colSums(is.na(df))
```
THere are no missing values
###checking for duplicated values and removing them
```{r}
duplicated_rows <- df[duplicated(df),]
df <- unique(df)
dim(df)

```
The are no duplicate values
###selecting numerical 
```{r}
num <- dplyr::select_if(df, is.numeric)
head(num)
```
##PCA
###get zero variance data
```{r}
which(apply(num, 2, var)==0)
```
###And to remove zero variance columns from the dataset
```{r}
num[ , which(apply(num, 2, var) != 0)]
```
###We then pass df to the prcomp()----
```{r}
df.pca <- prcomp(num [,c(1,3,4,5,6,7)], center = TRUE, scale. = TRUE)
summary(df.pca)
```
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE, scale. = TRUE)
summary(mtcars.pca)

PC1 explains 60% of the total variance, which means that more than half
 of the information in the dataset (11 variables) can be encapsulated 
 by just that one Principal Component. PC2 explains 24% of the variance.
 
# Calling str() to have a look at your PCA object
# ---
# 
```{r}
str(df.pca)
```

 we note that our pca object: The center point ($center), scaling ($scale), standard deviation(sdev) of each principal component. 
The relationship (correlation or anticorrelation, etc) between the initial variables and the principal components ($rotation). 
The values of each sample in terms of the principal components ($x)
###We will now plot our pca.
### Installing our ggbiplot visualisation package
```{r}
# plot method
plot(df.pca, type = "l")
```
# summary method
Importance of components:
```{r}
summary(df.pca)


```
```{r}
library(devtools)
install_github("ggbiplot", "vqv")

library(ggbiplot)
g <- ggbiplot(df.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE, 
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)
```

```{r}
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(df.pca)
```

```{r}
ggbiplot(df.pca, labels=rownames(df), obs.scale = 1, var.scale = 1)
```

```{r}
ggbiplot(df.pca,ellipse=TRUE,  labels=rownames(df), groups=df.grossincome, obs.scale = 1, var.scale = 1)
```
# We now plot PC3 and PC4
```{r}
ggbiplot(df.pca,ellipse=TRUE,choices=c(3,4),   labels=rownames(df), groups=df.grossincome)

```



#Association rule
```{r}
# We first we install the required arules library 
#
install.packages("arules")
```


```{r}
# Loading the arules library
#
library(arules)
```

# Loading our transactions dataset from our csv file
```{r}
path <-"http://bit.ly/SupermarketDatasetII"

Transactions<-read.transactions(path, sep = ",")
Transactions
```
# Verifying the object's class
```{r}
class(Transactions)
```

# Previewing our first 5 transactions
#
```{r}
inspect(Transactions[1:5])
```
# Generating a summary of the transaction dataset
```{r}
summary(Transactions)
```
# Exploring the frequency of some articles 
```{r}
itemFrequency(Transactions[, 8:10],type = "absolute")
round(itemFrequency(Transactions[, 8:10],type = "relative")*100,2)
```
# Producing a chart of frequencies and fitering 
```{r}
par(mfrow = c(1, 2))
```
# plot the frequency of items
```{r}
itemFrequencyPlot(Transactions, topN = 10,col="darkgreen")
itemFrequencyPlot(Transactions, support = 0.1,col="darkred")
```
# Building a model based on association rules 
# using the apriori function 
```{r}
rules <- apriori (Transactions, parameter = list(supp = 0.001, conf = 0.8))
rules
```
WE obtain set 0f 74 rules
# Building a apriori model with Min Support as 0.002 and confidence as 0.8.
```{r}
rules2 <- apriori (Transactions,parameter = list(supp = 0.002, conf = 0.8)) 
```
# Building apriori model with Min Support as 0.002 and confidence as 0.6.
```{r}
rules3 <- apriori (Transactions, parameter = list(supp = 0.001, conf = 0.6)) 
```
rule length distribution (lhs + rhs):sizes
 3  4  5  6 
15 42 16  1 
#to perform exploration of our model
```{r}
summary(rules)
```
summary of quality measures:
    support           confidence        coverage             lift            count       
 Min.   :0.001067   Min.   :0.8000   Min.   :0.001067   Min.   : 3.356   Min.   : 8.000  
 1st Qu.:0.001067   1st Qu.:0.8000   1st Qu.:0.001333   1st Qu.: 3.432   1st Qu.: 8.000  
 Median :0.001133   Median :0.8333   Median :0.001333   Median : 3.795   Median : 8.500  
 Mean   :0.001256   Mean   :0.8504   Mean   :0.001479   Mean   : 4.823   Mean   : 9.419  
 3rd Qu.:0.001333   3rd Qu.:0.8889   3rd Qu.:0.001600   3rd Qu.: 4.877   3rd Qu.:10.000  
 Max.   :0.002533   Max.   :1.0000   Max.   :0.002666   Max.   :12.722   Max.   :19.000  
# Observing rules built in our model i.e. first 5 model rules
# ---
# 
```{r}
inspect(rules[1:5])
```
1. If someone buys frozen smoothie and spinach. they are 89% likely to buy mineral water. 
2. If someone buys bacon and pancakes, they are 81% likley to buy spaghetti

# Ordering these rules by a criteria such as the level of confidence
# then looking at the first five rules.
```{r}
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
```
The first four rules have a confidence of 100%



#Anomaly detection
#Loading the dataset
```{r}
library(readr)
Supermarket_Sales_Forecasting_Sales <- read_csv("Supermarket_Sales_Forecasting - Sales.csv")
View(Supermarket_Sales_Forecasting_Sales)
```

